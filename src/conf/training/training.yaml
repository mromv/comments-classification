_target_: transformers.TrainingArguments
num_train_epochs: 20
evaluation_strategy: epoch
save_strategy: epoch
learning_rate: 2e-5
per_device_train_batch_size: 16
per_device_eval_batch_size: ${training.per_device_train_batch_size}
weight_decay: 0.01
load_best_model_at_end: True
metric_for_best_model: f1
warmup_steps: 400
logging_dir: ./logs
logging_steps: 500
greater_is_better: True
no_cuda: False
dataloader_pin_memory: False
label_names: [labels]
seed: ${general.random_state}